{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Myocardial Infraction Complications Analysis",
   "id": "78a3e83d9f440fc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Introduction",
   "id": "d152642d63a4220b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "229f39d26daf6f9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"MI.data\", header=None)\n",
    "data.columns = [\"ID\", \"AGE\", \"SEX\", \"INF_ANAM\", \"STENOK_AN\", \"FK_STENOK\", \"IBS_POST\", \"IBS_NASL\", \"GB\", \"SIM_GIPERT\", \"DLIT_AG\", \"ZSN_A\", \"nr_11\", \"nr_01\", \"nr_02\", \"nr_03\", \"nr_04\", \"nr_07\", \"nr_08\", \"np_01\", \"np_04\", \"np_05\", \"np_07\", \"np_08\", \"np_09\", \"np_10\", \"endocr_01\", \"endocr_02\", \"endocr_03\", \"zab_leg_01\", \"zab_leg_02\", \"zab_leg_03\", \"zab_leg_04\", \"zab_leg_06\", \"S_AD_KBRIG\", \"D_AD_KBRIG\", \"S_AD_ORIT\", \"D_AD_ORIT\", \"O_L_POST\", \"K_SH_POST\", \"MP_TP_POST\", \"SVT_POST\", \"GT_POST\", \"FIB_G_POST\", \"ant_im\", \"lat_im\", \"inf_im\", \"post_im\", \"IM_PG_P\", \"ritm_ecg_p_01\", \"ritm_ecg_p_02\", \"ritm_ecg_p_04\", \"ritm_ecg_p_06\", \"ritm_ecg_p_07\", \"ritm_ecg_p_08\", \"n_r_ecg_p_01\", \"n_r_ecg_p_02\", \"n_r_ecg_p_03\", \"n_r_ecg_p_04\", \"n_r_ecg_p_05\", \"n_r_ecg_p_06\", \"n_r_ecg_p_08\", \"n_r_ecg_p_09\", \"n_r_ecg_p_10\", \"n_p_ecg_p_01\", \"n_p_ecg_p_03\", \"n_p_ecg_p_04\", \"n_p_ecg_p_05\", \"n_p_ecg_p_06\", \"n_p_ecg_p_07\", \"n_p_ecg_p_08\", \"n_p_ecg_p_09\", \"n_p_ecg_p_10\", \"n_p_ecg_p_11\", \"n_p_ecg_p_12\", \"fibr_ter_01\", \"fibr_ter_02\", \"fibr_ter_03\", \"fibr_ter_05\", \"fibr_ter_06\", \"fibr_ter_07\", \"fibr_ter_08\", \"GIPO_K\", \"K_BLOOD\", \"GIPER_NA\", \"NA_BLOOD\", \"ALT_BLOOD\", \"AST_BLOOD\", \"KFK_BLOOD\", \"L_BLOOD\", \"ROE\", \"TIME_B_S\", \"R_AB_1_n\", \"R_AB_2_n\", \"R_AB_3_n\", \"NA_KB\", \"NOT_NA_KB\", \"LID_KB\", \"NITR_S\", \"NA_R_1_n\", \"NA_R_2_n\", \"NA_R_3_n\", \"NOT_NA_1_n\", \"NOT_NA_2_n\", \"NOT_NA_3_n\", \"LID_S_n\", \"B_BLOK_S_n\", \"ANT_CA_S_n\", \"GEPAR_S_n\", \"ASP_S_n\", \"TIKL_S_n\", \"TRENT_S_n\", \"FIBR_PREDS\", \"PREDS_TAH\", \"JELUD_TAH\", \"FIBR_JELUD\", \"A_V_BLOK\", \"OTEK_LANC\", \"RAZRIV\", \"DRESSLER\", \"ZSN\", \"REC_IM\", \"P_IM_STEN\", \"LET_IS\"]\n",
    "data.replace(\"?\", np.NaN, inplace=True)\n",
    "data = data.apply(pd.to_numeric, errors = \"coerce\")\n",
    "data"
   ],
   "id": "4a282e867634e1e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "related_time_features = [\"R_AB_1_n\",\"R_AB_2_n\", \"R_AB_3_n\", \"NA_R_1_n\", \"NA_R_2_n\", \"NA_R_3_n\", \"NOT_NA_1_n\",\"NOT_NA_2_n\", \"NOT_NA_3_n\"]",
   "id": "f0da78d177a97b24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Train-Validation-Test Split",
   "id": "7f2e380395edc9eb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.iloc[:, 1:112]\n",
    "X = X.drop(related_time_features, axis=1)\n",
    "y = [data[\"ZSN\"], data[\"FIBR_PREDS\"], data[\"P_IM_STEN\"], data[\"REC_IM\"], data[\"OTEK_LANC\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "# ! Caution\n",
    "# i = 0 => ZSN or Chronic heart failure\n",
    "# i = 1 => FIBR_PREDS or Atrial fibrillation\n",
    "# i = 2 => P_IM_STEN or Post-infarction angina\n",
    "# i = 3 => REC_IM or Relapse of the myocardial infarction\n",
    "# i = 4 => OTEK_LANC or Pulmonary edema\n",
    "\n",
    "for i in range(len(y)):\n",
    "    # Separate train/test split for each target variable\n",
    "    # Split the data into train and temporary sets\n",
    "    X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X, y[i], train_size=0.7, random_state=0, stratify=y[i])\n",
    "    \n",
    "    X_train.append(X_train_i)\n",
    "    X_test.append(X_test_i)\n",
    "    y_train.append(y_train_i)\n",
    "    y_test.append(y_test_i)\n"
   ],
   "id": "bfc1f11968db12a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the number of samples in each set and print as a table\n",
    "results = {\"Set\": [\"Train\", \"Test\"]}\n",
    "for i in range(len(y)):\n",
    "    results[f\"y{i}\"] = [len(y_train[i]), len(y_test[i])]\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ],
   "id": "859f29bc7e1f883",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. EDA",
   "id": "74b36820661d0c4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Overview",
   "id": "44fca05ecc8ec701"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "77e9c91df6f589ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Target Balance Check",
   "id": "da48e701111b4c89"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# The number of instances in the dataset\n",
    "number_of_instances = len(data)\n",
    "# Create a table to show the balance of each target variable\n",
    "results = {\"Target\": [], \"0\": [], \"1\": []}\n",
    "for i in range(len(y)):\n",
    "    results[\"Target\"].append(f\"y{i}\")\n",
    "    results[\"0\"].append(y[i].value_counts()[0] / number_of_instances)\n",
    "    results[\"1\"].append(y[i].value_counts()[1] / number_of_instances)\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ],
   "id": "dd81180b32aae85a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check for missing values",
   "id": "e4f076caf51ef9d1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a table representing number of missing values of each feature, sort them descendingly\n",
    "missing_values = X.isnull().sum().sort_values(ascending=False)\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values = pd.DataFrame(missing_values, columns=[\"Number of Missing Values\"])\n",
    "missing_values\n"
   ],
   "id": "27c9b86ec54f7186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Information of Feature",
   "id": "786074d78fd74c46"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "X.describe()",
   "id": "649fb07dc46f4195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "continuous_features = [\"AGE\", \"S_AD_ORIT\", \"D_AD_ORIT\", \"K_BLOOD\", \"NA_BLOOD\", \"ALT_BLOOD\", \"AST_BLOOD\", \"L_BLOOD\", \"ROE\"]\n",
    "binary_categorical_features = [x for x in X.columns if x not in continuous_features]"
   ],
   "id": "58460f4aba80d1a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "binary_features = []\n",
    "for col in binary_categorical_features:\n",
    "    if  X[col].max() == 1:\n",
    "        binary_features.append(col)"
   ],
   "id": "fe366ae73d2bde67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "categorical_features = [x for x in binary_categorical_features if x not in binary_features]",
   "id": "a56df190e54879be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Preprocessing",
   "id": "f1ee12f382218985"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "unprocessed_data = X_train\n",
    "%store unprocessed_data\n",
    "unprocessed_data[0]"
   ],
   "id": "904d86dc08062fa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold):\n",
    "        self.columns_to_drop = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.columns_to_drop = X.columns[X.isnull().sum() > self.threshold]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure the input is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        global X_keep \n",
    "        X_keep = list(X.columns[~X.columns.isin(self.columns_to_drop)])\n",
    "        return X.drop(columns=self.columns_to_drop)\n",
    "    \n",
    "\n",
    "class RowDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold):\n",
    "        self.rows_to_drop = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.rows_to_drop = X.index[X.isnull().sum(axis=1) > self.threshold]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure the input is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        return X.drop(index=self.rows_to_drop)\n",
    "    "
   ],
   "id": "9498ca38473f563f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Define outliers handler class\n",
    "class OutliersHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, coefficient):\n",
    "        self.coefficient = coefficient\n",
    "        self.lower_bounds = None\n",
    "        self.upper_bounds = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        q1 = X.quantile(0.25)\n",
    "        q3 = X.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        self.lower_bounds = q1 - iqr * self.coefficient\n",
    "        self.upper_bounds = q3 + iqr * self.coefficient\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure the input is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        for feature in X.columns:\n",
    "            if feature in continuous_features: # if feature is continuous, because clipping in the other one will probably result in missing information\n",
    "                X[feature] = X[feature].clip(self.lower_bounds[feature], self.upper_bounds[feature])\n",
    "        return X"
   ],
   "id": "db10f91164257ebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def impute_by_type(X, continuous_features=None, categorical_features=None, binary_features=None, binary_categorical_features=None):\n",
    "    \"\"\"Fills missing values based on data type, handling potential errors, works with subsets\"\"\"\n",
    "    if categorical_features is not None:\n",
    "        for feature in categorical_features:\n",
    "            if feature in X.columns:\n",
    "                try:\n",
    "                    X[feature].fillna(X[feature].mode()[0], inplace=True)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    if continuous_features is not None:\n",
    "        for feature in continuous_features:\n",
    "            if feature in X.columns:\n",
    "                try:\n",
    "                    X[feature].fillna(X[feature].mean(axis=0), inplace=True)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    if binary_features is not None:\n",
    "        for feature in binary_features:\n",
    "            if feature in X.columns:\n",
    "                try:\n",
    "                    X[feature].fillna(X[feature].mode()[0], inplace=True)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    if binary_categorical_features is not None and binary_features is None and categorical_features is None:\n",
    "        for feature in binary_categorical_features:\n",
    "            if feature in X.columns:\n",
    "                try:\n",
    "                    X[feature].fillna(X[feature].mode()[0], inplace=True)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    return X"
   ],
   "id": "1bf5b84d60a3601a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.impute import KNNImputer\n",
    "# def impute_by_type(X, continuous_features=None, categorical_features=None, binary_features=None):\n",
    "#     \"\"\"Fills missing values based on data type, handling potential errors, works with subsets\"\"\"\n",
    "#     if categorical_features is not None:\n",
    "#         for feature in categorical_features:\n",
    "#             if feature in X.columns:\n",
    "#                 try:\n",
    "#                     X[feature].fillna(X[feature].mode()[0], inplace=True)\n",
    "#                 except KeyError:\n",
    "#                     pass\n",
    "#     imputer = KNNImputer(n_neighbors=5)\n",
    "#     if continuous_features is not None:\n",
    "#         try:\n",
    "#             X_continuous = pd.DataFrame(imputer.fit_transform(X[continuous_features]))\n",
    "#             X.update(X_continuous)\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#     if binary_features is not None:\n",
    "#         try:\n",
    "#             X_binary = pd.DataFrame(imputer.fit_transform(X[binary_features]))\n",
    "#             X.update(X_binary)\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#     return X"
   ],
   "id": "aa6ace2834dca674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define dropper pipeline\n",
    "dropper = Pipeline(steps=[\n",
    "    ('column_dropper', ColumnDropper(threshold=100)),\n",
    "    ('row_dropper', RowDropper(threshold=100))\n",
    "])\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('dropper', dropper),\n",
    "    ('outliers_clipper',OutliersHandler(coefficient=1.5)),\n",
    "    ('imputation', FunctionTransformer(impute_by_type, kw_args={\"continuous_features\":continuous_features, \"binary_features\": binary_features, \"categorical_features\": categorical_features })),  # Fill missing values using mean/mode\n",
    "    ('scaling', StandardScaler())  # Standardize features by removing the mean and scaling to unit variance\n",
    "])\n",
    "\n",
    "# Apply the preprocessing pipeline to each set\n",
    "for i in range(len(y)):\n",
    "    X_train[i] = preprocessing_pipeline.fit_transform(X_train[i])\n",
    "    X_test[i] = preprocessing_pipeline.transform(X_test[i])\n",
    "\n",
    "#%store X_train\n",
    "#%store y_train\n",
    "#%store X_test\n",
    "#%store y_test\n",
    "\n",
    "preprocessed_data = pd.DataFrame(data=X_train[0], columns=X_keep)\n",
    "%store preprocessed_data\n",
    "preprocessed_data"
   ],
   "id": "ce01c981c12b39fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Selection",
   "id": "7fa45f79e09c0802"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#create a pipeline to predict y using svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('model', SVC())\n",
    "])\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid= {'model__C': [0.1, 1, 10, 100, 1000], 'model__gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'model__kernel': ['rbf', 'linear']}\n",
    "\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid, scoring = 'roc_auc', cv=5, n_jobs=-1)\n",
    "\n",
    "# Define a list to store the best parameters\n",
    "best_params = []\n",
    "\n",
    "# Fit the grid search for each target variable\n",
    "for i in range(len(y)):\n",
    "    grid_search.fit(X_train[i], y_train[i])\n",
    "    # Print the best parameters\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(f\"Best parameters for y{i}: {grid_search.best_params_}\")\n",
    "    # Store the parameters\n",
    "    best_params.append(grid_search.best_params_)\n",
    "\n",
    "    "
   ],
   "id": "1aa4f204174e72ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Model Evaluation",
   "id": "9c8556d89be33a3b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Create 5 tables for evaluating 5 targets, each table contains 1 column representing the model, 5 rows representing the metrics, which are accuracy, precision, recall, f1-score, and ROC AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for i in range(len(y)):\n",
    "    results = {\"Metric\" : [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC AUC\"]}\n",
    "    # Set the best parameters for the model in the pipeline\n",
    "    model_pipeline.set_params(**best_params[i])\n",
    "\n",
    "    # Fit the model with the best parameters\n",
    "    model_pipeline.fit(X_train[i], y_train[i])\n",
    "\n",
    "    # Predict the target variable\n",
    "    y_pred = model_pipeline.predict(X_test[i])\n",
    "\n",
    "    # Calculate the metrics\n",
    "    accuracy = accuracy_score(y_test[i], y_pred)\n",
    "    precision = precision_score(y_test[i], y_pred)\n",
    "    recall = recall_score(y_test[i], y_pred)\n",
    "    f1 = f1_score(y_test[i], y_pred)\n",
    "    roc_auc = roc_auc_score(y_test[i], y_pred)\n",
    "\n",
    "    # Store the metrics\n",
    "    results[\"SVM\"] = [accuracy, precision, recall, f1, roc_auc]\n",
    "    results = pd.DataFrame(results)\n",
    "    print(\"-------------------------------------------------\")\n",
    "    if i==0:\n",
    "        print(\"Result for ZSN or Chronic heart failure\")\n",
    "    if i==1:\n",
    "        print(\"Result for FIBR_PREDS or Atrial fibrillation\")\n",
    "    if i==2:\n",
    "        print(\"Result for P_IM_STEN or Post-infarction angina\")\n",
    "    if i==3:\n",
    "        print(\"Result for REC_IM or Relapse of the myocardial infarction\")\n",
    "    if i==4:\n",
    "        print(\"Result for OTEK_LANC or Pulmonary edema\")\n",
    "    print(results)\n",
    "    del results\n",
    "    \n"
   ],
   "id": "74619559f37d6fa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "97b312f92c61747d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
